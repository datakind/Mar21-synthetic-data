{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "controlling-intellectual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTHANKS TO https://www.maskaravivek.com/post/gan-synthetic-data-generation/ for the code!\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "THANKS TO https://www.maskaravivek.com/post/gan-synthetic-data-generation/ for the code!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model\n",
    "!mkdir model/gan\n",
    "!wget https://storage.googleapis.com/synthea-public/synthea_sample_data_csv_apr2020.zip\n",
    "!unzip synthea_sample_data_csv_apr2020.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "single-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kdami\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abroad-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"patients.csv\"\n",
    "columns_to_drop = ['Id', 'BIRTHDATE', 'DEATHDATE', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'ADDRESS', 'LAST', 'SUFFIX', 'MAIDEN','LAT', 'LON']\n",
    "categorical_features = ['MARITAL', 'RACE', 'ETHNICITY', 'GENDER', 'BIRTHPLACE', 'CITY', 'STATE', 'COUNTY', 'ZIP']\n",
    "continuous_features = ['HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE']\n",
    "col1, col2 = 'num_of_doors', 'price'\n",
    "col_group_by = 'body_style'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorporated-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MARITAL', 'RACE', 'ETHNICITY', 'GENDER', 'BIRTHPLACE', 'CITY', 'STATE',\n",
      "       'COUNTY', 'ZIP', 'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "print(df.columns)\n",
    "for column in categorical_features:\n",
    "  df[column] = df[column].astype('category').cat.codes\n",
    "for column in continuous_features:\n",
    "  min = df[column].min()\n",
    "  max = df[column].max()\n",
    "  feature_bins = pd.cut(df[column], bins=np.linspace(min, max, 21), labels=False)\n",
    "  df.drop([column], axis=1, inplace=True)\n",
    "  df = pd.concat([df, feature_bins], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns] = PowerTransformer(method='yeo-johnson', standardize=True, copy=True).fit_transform(df[df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "strange-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "# pwt=pw.fit_transform(df[df.columns])\n",
    "# df[df.columns]=pwt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "homeless-pacific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1171, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "banned-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configuration\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 32\n",
    "\n",
    "log_step = 100\n",
    "# epochs = 5000+1\n",
    "epochs = 100\n",
    "learning_rate = 5e-4\n",
    "models_dir = 'model'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GAN():\n",
    "\n",
    "    def __init__(self, gan_args):\n",
    "        [self.batch_size, lr, self.noise_dim,\n",
    "         self.data_dim, layers_dim] = gan_args\n",
    "\n",
    "        self.generator = Generator(self.batch_size). \\\n",
    "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
    "\n",
    "        self.discriminator = Discriminator(self.batch_size). \\\n",
    "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
    "\n",
    "        optimizer = Adam(lr, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.noise_dim,))\n",
    "        record = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(record)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def get_data_batch(self, train, batch_size, seed=0):\n",
    "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
    "        # np.random.seed(seed)\n",
    "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
    "        # iterate through shuffled indices, so every sample gets covered evenly\n",
    "\n",
    "        start_i = (batch_size * seed) % len(train)\n",
    "        stop_i = start_i + batch_size\n",
    "        shuffle_seed = (batch_size * seed) // len(train)\n",
    "        np.random.seed(shuffle_seed)\n",
    "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
    "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
    "        x = train.loc[train_ix[start_i: stop_i]].values\n",
    "        return np.reshape(x, (batch_size, -1))\n",
    "\n",
    "    def train(self, data, train_arguments):\n",
    "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
    "\n",
    "        data_cols = data.columns\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            batch_data = self.get_data_batch(data, self.batch_size)\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            ##diff from original - I added steps=1 here after an error stating that the steps arg had to be\n",
    "            ##explicitly declared. Adding more steps generates more fake data, however the input\n",
    "            ##DF and the gen_data DF have to have the same number of samples, apparently.\n",
    "            ##I'm sure there's a way around that limitation, right?\n",
    "            gen_data = self.generator.predict(noise,steps=1)\n",
    "            \n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated events\n",
    "            if epoch % sample_interval == 0:\n",
    "                # Test here data generation step\n",
    "                # save model checkpoints\n",
    "                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
    "                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n",
    "                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n",
    "\n",
    "                # Here is generating the data\n",
    "                z = tf.random.normal((432, self.noise_dim))\n",
    "                gen_data = self.generator(z)\n",
    "                print('generated_data')\n",
    "\n",
    "    def save(self, path, name):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        model_path = os.path.join(path, name)\n",
    "        self.generator.save_weights(model_path)  # Load the generator\n",
    "        return\n",
    "\n",
    "    def load(self, path):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        self.generator = Generator(self.batch_size)\n",
    "        self.generator = self.generator.load_weights(path)\n",
    "        return self.generator\n",
    "\n",
    "\n",
    "class Generator():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def build_model(self, input_shape, dim, data_dim):\n",
    "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim, activation='relu')(input)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dense(dim * 4, activation='relu')(x)\n",
    "        x = Dense(data_dim)(x)\n",
    "        return Model(inputs=input, outputs=x)\n",
    "\n",
    "\n",
    "class Discriminator():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def build_model(self, input_shape, dim):\n",
    "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim * 4, activation='relu')(input)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "        return Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "significant-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "data_cols = df.columns\n",
    "\n",
    "\n",
    "#Define the GAN and training parameters\n",
    "df[data_cols] = df[data_cols]\n",
    "\n",
    "print(df.shape[1])\n",
    "\n",
    "gan_args = [batch_size, learning_rate, noise_dim, df.shape[1], dim]\n",
    "train_args = ['', epochs, log_step]\n",
    "\n",
    "\n",
    "model = GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "minus-charger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.713244, acc.: 26.56%] [G loss: 0.694922]\n",
      "generated_data\n",
      "1 [D loss: 0.669719, acc.: 48.44%] [G loss: 0.676665]\n",
      "2 [D loss: 0.651264, acc.: 50.00%] [G loss: 0.649757]\n",
      "3 [D loss: 0.662780, acc.: 50.00%] [G loss: 0.614664]\n",
      "4 [D loss: 0.657544, acc.: 50.00%] [G loss: 0.615067]\n",
      "5 [D loss: 0.635129, acc.: 50.00%] [G loss: 0.672701]\n",
      "6 [D loss: 0.589370, acc.: 50.00%] [G loss: 0.776516]\n",
      "7 [D loss: 0.560900, acc.: 60.94%] [G loss: 0.870814]\n",
      "8 [D loss: 0.566993, acc.: 56.25%] [G loss: 0.915446]\n",
      "9 [D loss: 0.593899, acc.: 48.44%] [G loss: 0.864770]\n",
      "10 [D loss: 0.641016, acc.: 48.44%] [G loss: 0.869567]\n",
      "11 [D loss: 0.628211, acc.: 48.44%] [G loss: 1.015116]\n",
      "12 [D loss: 0.551313, acc.: 79.69%] [G loss: 1.154352]\n",
      "13 [D loss: 0.496523, acc.: 90.62%] [G loss: 1.262675]\n",
      "14 [D loss: 0.480788, acc.: 92.19%] [G loss: 1.232287]\n",
      "15 [D loss: 0.497214, acc.: 89.06%] [G loss: 1.181813]\n",
      "16 [D loss: 0.514858, acc.: 81.25%] [G loss: 1.126976]\n",
      "17 [D loss: 0.540496, acc.: 67.19%] [G loss: 1.027967]\n",
      "18 [D loss: 0.535199, acc.: 78.12%] [G loss: 0.995299]\n",
      "19 [D loss: 0.518387, acc.: 81.25%] [G loss: 1.000446]\n",
      "20 [D loss: 0.540347, acc.: 76.56%] [G loss: 1.066011]\n",
      "21 [D loss: 0.545831, acc.: 73.44%] [G loss: 0.998288]\n",
      "22 [D loss: 0.622342, acc.: 46.88%] [G loss: 0.922036]\n",
      "23 [D loss: 0.581442, acc.: 51.56%] [G loss: 0.920444]\n",
      "24 [D loss: 0.519123, acc.: 82.81%] [G loss: 1.068057]\n",
      "25 [D loss: 0.450245, acc.: 90.62%] [G loss: 1.177575]\n",
      "26 [D loss: 0.398822, acc.: 90.62%] [G loss: 1.277274]\n",
      "27 [D loss: 0.395360, acc.: 95.31%] [G loss: 1.201449]\n",
      "28 [D loss: 0.454245, acc.: 82.81%] [G loss: 1.144560]\n",
      "29 [D loss: 0.515909, acc.: 73.44%] [G loss: 1.085108]\n",
      "30 [D loss: 0.486962, acc.: 71.88%] [G loss: 0.989327]\n",
      "31 [D loss: 0.535670, acc.: 67.19%] [G loss: 0.982652]\n",
      "32 [D loss: 0.534183, acc.: 68.75%] [G loss: 1.017229]\n",
      "33 [D loss: 0.490019, acc.: 84.38%] [G loss: 1.098319]\n",
      "34 [D loss: 0.487776, acc.: 87.50%] [G loss: 1.150782]\n",
      "35 [D loss: 0.416808, acc.: 92.19%] [G loss: 1.220934]\n",
      "36 [D loss: 0.428325, acc.: 90.62%] [G loss: 1.185924]\n",
      "37 [D loss: 0.433843, acc.: 84.38%] [G loss: 1.179387]\n",
      "38 [D loss: 0.429220, acc.: 85.94%] [G loss: 1.226259]\n",
      "39 [D loss: 0.432813, acc.: 87.50%] [G loss: 1.135609]\n",
      "40 [D loss: 0.440942, acc.: 82.81%] [G loss: 1.135236]\n",
      "41 [D loss: 0.459340, acc.: 78.12%] [G loss: 1.214288]\n",
      "42 [D loss: 0.451576, acc.: 81.25%] [G loss: 1.263165]\n",
      "43 [D loss: 0.448198, acc.: 82.81%] [G loss: 1.358659]\n",
      "44 [D loss: 0.387508, acc.: 93.75%] [G loss: 1.347562]\n",
      "45 [D loss: 0.407638, acc.: 89.06%] [G loss: 1.318897]\n",
      "46 [D loss: 0.418927, acc.: 92.19%] [G loss: 1.244409]\n",
      "47 [D loss: 0.433788, acc.: 87.50%] [G loss: 1.293247]\n",
      "48 [D loss: 0.422481, acc.: 84.38%] [G loss: 1.228273]\n",
      "49 [D loss: 0.465887, acc.: 84.38%] [G loss: 1.219939]\n",
      "50 [D loss: 0.463656, acc.: 87.50%] [G loss: 1.245206]\n",
      "51 [D loss: 0.449111, acc.: 89.06%] [G loss: 1.311818]\n",
      "52 [D loss: 0.414401, acc.: 84.38%] [G loss: 1.497840]\n",
      "53 [D loss: 0.360817, acc.: 87.50%] [G loss: 1.503737]\n",
      "54 [D loss: 0.395696, acc.: 84.38%] [G loss: 1.517924]\n",
      "55 [D loss: 0.334190, acc.: 89.06%] [G loss: 1.662912]\n",
      "56 [D loss: 0.351450, acc.: 89.06%] [G loss: 1.762219]\n",
      "57 [D loss: 0.332390, acc.: 93.75%] [G loss: 1.594154]\n",
      "58 [D loss: 0.324299, acc.: 95.31%] [G loss: 1.489859]\n",
      "59 [D loss: 0.343233, acc.: 92.19%] [G loss: 1.538449]\n",
      "60 [D loss: 0.307298, acc.: 95.31%] [G loss: 1.547148]\n",
      "61 [D loss: 0.265053, acc.: 95.31%] [G loss: 1.568364]\n",
      "62 [D loss: 0.237958, acc.: 96.88%] [G loss: 1.788500]\n",
      "63 [D loss: 0.259510, acc.: 93.75%] [G loss: 1.689432]\n",
      "64 [D loss: 0.327803, acc.: 90.62%] [G loss: 1.964983]\n",
      "65 [D loss: 0.230372, acc.: 95.31%] [G loss: 1.830567]\n",
      "66 [D loss: 0.281499, acc.: 87.50%] [G loss: 1.703102]\n",
      "67 [D loss: 0.370379, acc.: 89.06%] [G loss: 1.717090]\n",
      "68 [D loss: 0.320162, acc.: 90.62%] [G loss: 1.621799]\n",
      "69 [D loss: 0.376057, acc.: 90.62%] [G loss: 1.653771]\n",
      "70 [D loss: 0.414835, acc.: 84.38%] [G loss: 1.812016]\n",
      "71 [D loss: 0.480653, acc.: 76.56%] [G loss: 1.725090]\n",
      "72 [D loss: 0.532832, acc.: 76.56%] [G loss: 1.968939]\n",
      "73 [D loss: 0.444926, acc.: 78.12%] [G loss: 1.590869]\n",
      "74 [D loss: 0.623649, acc.: 70.31%] [G loss: 2.038300]\n",
      "75 [D loss: 0.555147, acc.: 71.88%] [G loss: 2.266581]\n",
      "76 [D loss: 0.426794, acc.: 82.81%] [G loss: 2.682221]\n",
      "77 [D loss: 0.409353, acc.: 84.38%] [G loss: 2.537433]\n",
      "78 [D loss: 0.389762, acc.: 84.38%] [G loss: 2.555696]\n",
      "79 [D loss: 0.361137, acc.: 87.50%] [G loss: 2.678159]\n",
      "80 [D loss: 0.309693, acc.: 90.62%] [G loss: 2.402003]\n",
      "81 [D loss: 0.378259, acc.: 87.50%] [G loss: 2.206321]\n",
      "82 [D loss: 0.307730, acc.: 90.62%] [G loss: 2.016552]\n",
      "83 [D loss: 0.350491, acc.: 84.38%] [G loss: 1.937605]\n",
      "84 [D loss: 0.307629, acc.: 90.62%] [G loss: 1.883291]\n",
      "85 [D loss: 0.417173, acc.: 81.25%] [G loss: 1.843364]\n",
      "86 [D loss: 0.511099, acc.: 71.88%] [G loss: 1.440023]\n",
      "87 [D loss: 0.753186, acc.: 65.62%] [G loss: 1.737062]\n",
      "88 [D loss: 0.731880, acc.: 68.75%] [G loss: 1.851980]\n",
      "89 [D loss: 0.592162, acc.: 68.75%] [G loss: 2.225242]\n",
      "90 [D loss: 0.617958, acc.: 75.00%] [G loss: 2.174058]\n",
      "91 [D loss: 0.600252, acc.: 71.88%] [G loss: 2.379316]\n",
      "92 [D loss: 0.522569, acc.: 79.69%] [G loss: 2.183025]\n",
      "93 [D loss: 0.599960, acc.: 73.44%] [G loss: 2.300144]\n",
      "94 [D loss: 0.532109, acc.: 78.12%] [G loss: 2.508052]\n",
      "95 [D loss: 0.516351, acc.: 79.69%] [G loss: 2.538359]\n",
      "96 [D loss: 0.360333, acc.: 85.94%] [G loss: 2.353971]\n",
      "97 [D loss: 0.557227, acc.: 73.44%] [G loss: 2.567049]\n",
      "98 [D loss: 0.289833, acc.: 87.50%] [G loss: 2.579500]\n",
      "99 [D loss: 0.320805, acc.: 87.50%] [G loss: 2.198643]\n"
     ]
    }
   ],
   "source": [
    "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
    "synthesizer = model(gan_args)\n",
    "synthesizer.train(df, train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "growing-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.save('model/gan/', 'generator_patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "processed-pitch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(32, 32)]                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             multiple                  4224      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             multiple                  33024     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             multiple                  131584    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             multiple                  5643      \n",
      "=================================================================\n",
      "Total params: 174,475\n",
      "Trainable params: 174,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "synthesizer.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "banner-kitchen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(32, 11)]                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             multiple                  6144      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             multiple                  131328    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             multiple                  129       \n",
      "=================================================================\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Total params: 340,994\n",
      "Trainable params: 170,497\n",
      "Non-trainable params: 170,497\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "synthesizer.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "overall-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'GAN': ['GAN', False, synthesizer.generator]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "prospective-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "reverse-channel",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 748 into shape (946,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-404bc174ac29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthesizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mreal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-d8cab5474b9e>\u001b[0m in \u001b[0;36mget_data_batch\u001b[0;34m(self, train, batch_size, seed)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrain_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# duplicate to cover ranges past the end of the set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstop_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_arguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    297\u001b[0m            [5, 6]])\n\u001b[1;32m    298\u001b[0m     \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 748 into shape (946,newaxis)"
     ]
    }
   ],
   "source": [
    "###NOT WORKING!\n",
    "\n",
    "# Setup parameters visualization parameters\n",
    "seed = 17\n",
    "test_size = 946\n",
    "noise_dim = 32\n",
    "\n",
    "np.random.seed(seed)\n",
    "z = np.random.normal(size=(test_size, noise_dim))\n",
    "real = synthesizer.get_data_batch(train=df, batch_size=test_size, seed=seed)\n",
    "real_samples = pd.DataFrame(real, columns=data_cols)\n",
    "\n",
    "model_names = ['GAN']\n",
    "colors = ['deepskyblue','blue']\n",
    "markers = ['o','^']\n",
    "\n",
    "base_dir = 'model/'\n",
    "\n",
    "#Actual fraud data visualization\n",
    "model_steps = [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n",
    "rows = len(model_steps)\n",
    "columns = 5\n",
    "\n",
    "axarr = [[]]*len(model_steps)\n",
    "\n",
    "fig = plt.figure(figsize=(14,rows*3))\n",
    "\n",
    "for model_step_ix, model_step in enumerate(model_steps):        \n",
    "    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n",
    "    \n",
    "    for group, color, marker in zip(real_samples.groupby(col_group_by), colors, markers):\n",
    "        plt.scatter( group[1][[col1]], group[1][[col2]], marker=marker, edgecolors=color, facecolors='none' )\n",
    "    \n",
    "    plt.title('Actual Patients Data')\n",
    "    plt.ylabel(col2) # Only add y label to left plot\n",
    "    plt.xlabel(col1)\n",
    "    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()\n",
    "    \n",
    "    if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    i=0\n",
    "    [model_name, with_class, generator_model] = models['GAN']\n",
    "\n",
    "    generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
    "\n",
    "    ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
    "\n",
    "    g_z = generator_model.predict(z)\n",
    "\n",
    "    gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "    gen_samples.to_csv('Generated_sample.csv')\n",
    "    plt.scatter( gen_samples[[col1]], gen_samples[[col2]], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n",
    "    plt.title(\"Generated Data\")   \n",
    "    plt.xlabel(data_cols[0])\n",
    "    ax.set_xlim(xlims), ax.set_ylim(ylims)\n",
    "\n",
    "plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0.075,0,1,0.95])\n",
    "\n",
    "# Adding text labels for traning steps\n",
    "vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n",
    "vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n",
    "for model_step_ix, model_step in enumerate( model_steps ):\n",
    "    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n",
    "\n",
    "plt.savefig('Comparison_of_GAN_outputs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-holiday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
