{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "THANKS TO https://www.maskaravivek.com/post/gan-synthetic-data-generation/ for the code!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model\n",
    "!mkdir model/gan\n",
    "# !wget https://storage.googleapis.com/synthea-public/synthea_sample_data_csv_apr2020.zip\n",
    "# !unzip synthea_sample_data_csv_apr2020.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"csv/Datakind Sample Data - assessment.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['patient_id','patient_name','p_note','reported','num_satchets','micronutrient']\n",
    "continuous_features = ['patient_age_in_days','breath_count','patient_temperature','muac_score','child_weight']\n",
    "categorical_features = [i for i in list(df.columns) if i not in (continuous_features +columns_to_drop)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-spring",
   "metadata": {},
   "source": [
    "We may need to do some n/a and 0 substitution before creating the bins and converting with PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-sussex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-construction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "for column in categorical_features:\n",
    "  df[column] = df[column].astype('category').cat.codes\n",
    "for column in continuous_features:\n",
    "  min = df[column].min()\n",
    "  max = df[column].max()\n",
    "  feature_bins = pd.cut(df[column], bins=np.linspace(min, max, 21), labels=False)\n",
    "  df.drop([column], axis=1, inplace=True)\n",
    "  df = pd.concat([df, feature_bins], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-upset",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df.columns] = PowerTransformer(method='yeo-johnson', standardize=True, copy=True).fit_transform(df[df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "\n",
    "    def __init__(self, gan_args):\n",
    "        [self.batch_size, lr, self.noise_dim,\n",
    "         self.data_dim, layers_dim] = gan_args\n",
    "\n",
    "        self.generator = Generator(self.batch_size). \\\n",
    "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
    "\n",
    "        self.discriminator = Discriminator(self.batch_size). \\\n",
    "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
    "\n",
    "        optimizer = Adam(lr, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "        self.discriminator.trainable = False\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.noise_dim,))\n",
    "        record = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(record)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def get_data_batch(self, train, batch_size, seed=0):\n",
    "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
    "        # np.random.seed(seed)\n",
    "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
    "        # iterate through shuffled indices, so every sample gets covered evenly\n",
    "\n",
    "        start_i = (batch_size * seed) % len(train)\n",
    "        stop_i = start_i + batch_size\n",
    "        shuffle_seed = (batch_size * seed) // len(train)\n",
    "        np.random.seed(shuffle_seed)\n",
    "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
    "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
    "        x = train.loc[train_ix[start_i: stop_i]].values\n",
    "        return np.reshape(x, (batch_size, -1))\n",
    "\n",
    "    def train(self, data, train_arguments):\n",
    "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
    "\n",
    "        data_cols = data.columns\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((self.batch_size, 1))\n",
    "        fake = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            batch_data = self.get_data_batch(data, self.batch_size)\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            ##diff from original - I added steps=1 here after an error stating that the steps arg had to be\n",
    "            ##explicitly declared. Adding more steps generates more fake data, however the input\n",
    "            ##DF and the gen_data DF have to have the same number of samples, apparently.\n",
    "            ##I'm sure there's a way around that limitation, right?\n",
    "            gen_data = self.generator.predict(noise,steps=1)\n",
    "            \n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated events\n",
    "            if epoch % sample_interval == 0:\n",
    "                # Test here data generation step\n",
    "                # save model checkpoints\n",
    "                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.tf'\n",
    "                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch),save_format='tf')\n",
    "                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch),save_format='tf')\n",
    "\n",
    "                # Here is generating the data\n",
    "                z = tf.random.normal((432, self.noise_dim))\n",
    "                gen_data = self.generator(z)\n",
    "                print('generated_data')\n",
    "\n",
    "    def save(self, path, name):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        model_path = os.path.join(path, name)\n",
    "        self.generator.save_weights(model_path,save_format='tf')  # Load the generator\n",
    "        return\n",
    "\n",
    "    def load(self, path):\n",
    "        assert os.path.isdir(path) == True, \\\n",
    "            \"Please provide a valid path. Path must be a directory.\"\n",
    "        self.generator = Generator(self.batch_size)\n",
    "        self.generator = self.generator.load_weights(path)\n",
    "        return self.generator\n",
    "\n",
    "\n",
    "class Generator():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def build_model(self, input_shape, dim, data_dim):\n",
    "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim, activation='relu')(input)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dense(dim * 4, activation='relu')(x)\n",
    "        x = Dense(data_dim)(x)\n",
    "        return Model(inputs=input, outputs=x)\n",
    "\n",
    "\n",
    "class Discriminator():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def build_model(self, input_shape, dim):\n",
    "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
    "        x = Dense(dim * 4, activation='relu')(input)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim * 2, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(dim, activation='relu')(x)\n",
    "        x = Dense(1, activation='sigmoid')(x)\n",
    "        return Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df[['patient_age_in_days','breath_count','fast_breathing']]\n",
    "target_df = small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configuration\n",
    "noise_dim = 32\n",
    "dim = 128\n",
    "batch_size = 10\n",
    "\n",
    "log_step = 100 #at every n epochs, save the weights to disc\n",
    "epochs = 500\n",
    "learning_rate = 5e-4\n",
    "models_dir = 'model'\n",
    "data_cols = target_df.columns\n",
    "\n",
    "\n",
    "#Define the GAN and training parameters\n",
    "target_df[data_cols] = target_df[data_cols]\n",
    "\n",
    "print(target_df.shape[1])\n",
    "\n",
    "gan_args = [batch_size, learning_rate, noise_dim, target_df.shape[1], dim]\n",
    "train_args = ['', epochs, log_step]\n",
    "\n",
    "\n",
    "model = GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-charger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
    "synthesizer = model(gan_args)\n",
    "synthesizer.train(target_df, train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.save('model/gan/', 'generator_patients_oops.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'GAN': ['GAN', False, synthesizer.generator]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters visualization parameters\n",
    "seed = 17\n",
    "test_size = 32\n",
    "noise_dim = 32\n",
    "# np.random.seed(seed)\n",
    "real = synthesizer.get_data_batch(train=target_df, batch_size=test_size, seed=seed)\n",
    "real_samples = pd.DataFrame(real, columns=data_cols)\n",
    "\n",
    "z = np.random.normal(size=(test_size, noise_dim))\n",
    "\n",
    "model_names = ['GAN']\n",
    "colors = ['deepskyblue','blue']\n",
    "markers = ['o','^']\n",
    "\n",
    "col1, col2 = 'patient_age_in_days', 'breath_count'\n",
    "\n",
    "base_dir = 'model/'\n",
    "\n",
    "#Actual patient data visualization\n",
    "model_steps = range(0,epochs,int(epochs / (epochs//10)))\n",
    "rows = len(model_steps)\n",
    "columns = 5\n",
    "\n",
    "axarr = [[]]*len(model_steps)\n",
    "\n",
    "fig = plt.figure(figsize=(14,rows*3))\n",
    "\n",
    "for model_step_ix, model_step in enumerate(model_steps):        \n",
    "    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n",
    "    \n",
    "    for group, color, marker in zip(real_samples.groupby('fast_breathing'), colors, markers):\n",
    "        plt.scatter( group[1][[col1]], group[1][[col2]], marker=marker, edgecolors=color, facecolors='none' )\n",
    "    \n",
    "    plt.title('Actual Patients Data')\n",
    "    plt.ylabel(col2) # Only add y label to left plot\n",
    "    plt.xlabel(col1)\n",
    "    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()\n",
    "    \n",
    "    if model_step_ix == 0: \n",
    "        legend = plt.legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    \n",
    "    i=0\n",
    "    [model_name, with_class, generator_model] = models['GAN']\n",
    "\n",
    "    generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.tf')\n",
    "\n",
    "    ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
    "\n",
    "    g_z = generator_model.predict(z)\n",
    "    gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
    "    gen_samples.to_csv('Generated_sample.csv')\n",
    "    plt.scatter( gen_samples[[col1]], gen_samples[[col2]], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n",
    "    plt.title(\"Generated Data\")   \n",
    "    plt.xlabel(col1)\n",
    "    ax.set_xlim(xlims), ax.set_ylim(ylims)\n",
    "\n",
    "plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0.075,0,1,0.95])\n",
    "\n",
    "# Adding text labels for traning steps\n",
    "vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n",
    "vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n",
    "for model_step_ix, model_step in enumerate( model_steps ):\n",
    "    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n",
    "\n",
    "plt.savefig('Comparison_of_GAN_outputs.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
